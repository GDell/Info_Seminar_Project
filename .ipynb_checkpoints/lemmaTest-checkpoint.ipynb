{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gabriel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_id = '24ac8855'\n",
    "app_key = 'baa032bb2944756dcf361a18ae7e3ab9'\n",
    "\n",
    "actual_dictionary = {}\n",
    "\n",
    "alphabet = set(list(string.ascii_lowercase) + list(string.ascii_uppercase))\n",
    "\n",
    "def find_defs(diction, posCategory):\n",
    "    \"\"\"\n",
    "    This function executes when we reach a list containing dictionary data strucutres that may contain several different\n",
    "    definitions for a given word. It runs through this list, for each list finding keys in the dictionary data\n",
    "    structure containing the string \"definitions\" or \"short_definitions\" and appending the corresponding value \n",
    "    (a string) of this key to a master string, which we call stringy. At the end of this function, we use the \n",
    "    built-in function set() on the list given to us by the .split() method. The reasons for a set are:\n",
    "    1) checking membership in sets is efficient in python\n",
    "    2) sets do not contain duplicate elements, so we are using less space.\n",
    "    \"\"\"\n",
    "    stringy = ''\n",
    "    for i in diction:\n",
    "        for k, v in i.items():\n",
    "            if k == 'definitions' or k == 'short_definitions' or k == 'crossReferenceMarkers':\n",
    "                stringy += v[0] + ' '\n",
    "    stringy = stringy.split(' ')\n",
    "    \n",
    "    for word in stringy:  \n",
    "        n = stringy.index(word)\n",
    "        \n",
    "        \n",
    "        if len(word) != 0:\n",
    "            tempZero = word[0] \n",
    "#             print(tempZero)\n",
    "            if word[0] not in alphabet:\n",
    "                stringy[n] = wordnet_lemmatizer.lemmatize(word[1:], pos=posCategory).lower()\n",
    "\n",
    "            if word[-1] not in alphabet:\n",
    "                stringy[n] = wordnet_lemmatizer.lemmatize(word[:-1], pos=posCategory).lower()\n",
    "\n",
    "    return set(stringy)\n",
    "\n",
    "def find_definition(word):\n",
    "    \"\"\"\n",
    "    This function takes in a word as input and creates an entry in our dataset consisting of a word-definition pair.\n",
    "    \"\"\"\n",
    "#     wordnet_lemmatizer.lemmatize(word)\n",
    "    \n",
    "    # there may be times while looking up words that the dictionary does not contain a word for whatever reason.\n",
    "    # this chunk of code catches this error.\n",
    "    try:\n",
    "#         lemmatizedWord = wordnet_lemmatizer.lemmatize(word).lower()\n",
    "        url = 'https://od-api.oxforddictionaries.com:443/api/v1/entries/en/' + word.lower()\n",
    "        r = requests.get(url, headers = {'app_id': app_id, 'app_key': app_key})\n",
    "        dicty = r.json()\n",
    "        # # Need to consider words like \"won\" that have atypical structure...........       \n",
    "        # this process is ugly, but it works. \n",
    "        # The structure of the data returned from the API call is a bit awkward.\n",
    "        first_layer = dicty['results']\n",
    "        \n",
    "        second_layer = first_layer[0]\n",
    "        \n",
    "#         print(second_layer)\n",
    "        \n",
    "        third_layer = second_layer['lexicalEntries']\n",
    "        \n",
    "#         lexCategory = third_layer['lexicalCategories']\n",
    "#         print(third_layer)\n",
    "        \n",
    "        # third layer contains multiple definitions or \"senses\" of the word\n",
    "        fourth_layer = third_layer[0]\n",
    "        lexCategory = fourth_layer['lexicalCategory']\n",
    "        print(lexCategory)\n",
    "        if lexCategory == \"Noun\":\n",
    "            lexCategory = \"n\"\n",
    "        if lexCategory == \"Verb\":\n",
    "            lexCategory = \"v\"\n",
    "        if lexCategory == \"Adjective\":\n",
    "            lexCategory = \"a\"\n",
    "        print(lexCategory)\n",
    "        \n",
    "       \n",
    "        \n",
    "        fifth_layer = fourth_layer['entries']\n",
    "        sixth_layer = fifth_layer[0]\n",
    "        seventh_layer = sixth_layer['senses']\n",
    "#         print(seventh_layer)\n",
    "        if find_defs(seventh_layer, lexCategory) != {''}:\n",
    "            actual_dictionary[wordnet_lemmatizer.lemmatize(word, pos=lexCategory).lower()] = find_defs(seventh_layer, lexCategory)\n",
    "        else:\n",
    "            pass\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"something fucked up with \" + word)\n",
    "        pass\n",
    "    \n",
    "def find_contents(diction):\n",
    "    try:\n",
    "        for k, v in diction.items():\n",
    "            print(k, '\\n', v, '\\n\\n')\n",
    "    except AttributeError:\n",
    "        for i in diction:\n",
    "            print(i, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining functions using \"Pickle\" library that allow us to save python objects as .pkl files that, when loaded again,\n",
    "# act just like the objects we saved them as without needing to transform them.\n",
    "\n",
    "# This allows us to save a dictionary instead of needing to reconsruct a dictionary for each info-theoretic calculation.\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_csv = pd.read_csv('Dictionaries/Cambridge_gk-scch.csv', sep=';')\n",
    "word_list = list(set(word_csv['WORD'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun\n",
      "n\n",
      "elephant \n",
      " {'', '58', 'prehensile', 'of', 'tusks', '2', '71', 'largest', 'and', 'asia', 'Africa', 'paper', 'ivory', 'the', 'living', 'typically', 'plant-eating', 'to', 'southern', 'is', 'ear', 'very', 'approximately', 'long', 'tusk', 'with', 'a', 'It', 'mm)', 'size', 'large', 'trunk', 'curved', 'animal', 'native', 'inches', 'land', 'mammal'} \n",
      "\n",
      "\n",
      "elephant \n",
      " {'', '58', 'prehensile', 'of', 'tusks', '2', '71', 'largest', 'and', 'asia', 'Africa', 'paper', 'ivory', 'living', 'typically', 'mammal', 'plant-eating', 'to', 'southern', 'is', 'ear', 'very', 'long', 'native', 'tusk', 'with', 'a', 'It', 'mm)', 'size', 'large', 'trunk', 'curved', 'animal', 'the', 'inches', 'land', 'approximately'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_definition('elephant')\n",
    "\n",
    "save_obj(actual_dictionary, 'dictionary')\n",
    "\n",
    "find_contents(actual_dictionary)\n",
    "\n",
    "dicty = load_obj('dictionary')\n",
    "\n",
    "find_contents(dicty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gabriel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[('r', 'NN'), ('u', 'JJ'), ('n', 'JJ'), ('n', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "checkWord = \"running\".lower()\n",
    "POS = nltk.pos_tag(checkWord)\n",
    "\n",
    "print(POS)\n",
    "# lemmatizedWord = wordnet_lemmatizer.lemmatize(checkWord, pos=POS)\n",
    "# print(lemmatizedWord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
